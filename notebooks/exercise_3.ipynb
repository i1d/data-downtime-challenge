{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Downtime Challenge | Exercise 3\n",
    "\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import re\n",
    "from datetime import datetime, date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.utils import get_days_index\n",
    "\n",
    "all_days = get_days_index(350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"../data/dbs/Ex3.db\")\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In the last exercise, we looked at incidents spanning multiple tables in a database. Yet, we've still only looked at _individual_ metrics like the row count, rate of null values, and so on. In practice, many genuine data downtime incidents involve _conjunctions_ of events across multiple upstream and downstream tables. In this exercise, we practice crafting single queries that can handle such conjunctive events.\n",
    "\n",
    "## 2. Data\n",
    "\n",
    "In this exercise, we'll continue to use the `EXOPLANETS`, `HABITABLES`, and `EXOPLANETS_SCHEMA` tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all tables in DB\n",
    "pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        NAME\n",
    "    FROM \n",
    "        SQLITE_MASTER \n",
    "    WHERE \n",
    "        TYPE ='table' AND \n",
    "        NAME NOT LIKE 'sqlite_%';\n",
    "    \"\"\",\n",
    "    conn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercise: Pillars of Data Observability in Conjunction\n",
    "\n",
    "Why care about conjunctions of events, when individual events provide all the information you need?\n",
    "\n",
    "One important factor is **noise** -- looking at simultaneous events reduces the total number of events you're worried about, and makes it more likely that the issues are genuine. Another factor is **causality** -- given an issue in some table, looking at simultaneous events in upstream tables might help you determine the root cause, and reveal the path to a solution.\n",
    "\n",
    "Take the past exercise as an example -- we saw that the `habitability` field had anomalous rates of zeroed values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_zero = pd.read_sql_query(\"\"\"\n",
    "SELECT\n",
    "    DATE_ADDED,\n",
    "    CAST(SUM(CASE WHEN HABITABILITY IS 0 THEN 1 ELSE 0 END) AS FLOAT) / COUNT(*) AS ZERO_RATE\n",
    "FROM\n",
    "    HABITABLES\n",
    "GROUP BY\n",
    "    DATE_ADDED\n",
    "\"\"\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_zero = h_zero \\\n",
    "    .rename(columns={clmn: clmn.lower() for clmn in h_zero.columns}) \\\n",
    "    .set_index(\"date_added\") \\\n",
    "    .reindex(all_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to detect these anomalous values. One simple approach could be to define a threshold, and alert whenever the zero rate exceeded that threshold. How about 30%, for now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_zero_alerts = pd.read_sql_query(\"\"\"\n",
    "WITH HABITABILITY_ZERO_RATE AS(\n",
    "    SELECT\n",
    "        DATE_ADDED,\n",
    "        CAST(SUM(CASE WHEN HABITABILITY IS 0 THEN 1 ELSE 0 END) AS FLOAT) / COUNT(*) AS ZERO_RATE\n",
    "    FROM\n",
    "        HABITABLES\n",
    "    GROUP BY\n",
    "        DATE_ADDED\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    DATE_ADDED\n",
    "FROM\n",
    "    HABITABILITY_ZERO_RATE\n",
    "WHERE\n",
    "    ZERO_RATE IS NOT NULL AND\n",
    "    ZERO_RATE > 0.3\n",
    "\"\"\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_zero_alerts = h_zero_alerts.rename(columns={clmn: clmn.lower() for clmn in h_zero_alerts.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(x=all_days, y=h_zero[\"zero_rate\"])\n",
    "for alert in h_zero_alerts[\"date_added\"]: fig.add_vline(x=alert, line_color='red')\n",
    "fig.update_xaxes(title=\"Date\")\n",
    "fig.update_yaxes(title=\"Habitability Zero Rate\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly identifies some problematic timestamps, but it's *way too noisy*. We wouldn't want a notification for every red line on the above graph. Looking for other (upstream) events can not only prune our alerts, but also help us identify the issue's cause.\n",
    "\n",
    "See if you can `JOIN` the timestamps from `h_zero_alerts` with timestamps identifying a `schema_change`. For a refresher on `JOIN`ing in SQLite, check out [this link](https://www.sqlitetutorial.net/sqlite-inner-join/).\n",
    "\n",
    "*Hint*: try querying the `EXOPLANETS_SCHEMA` table using the same approach from Exercise 2. For reference, the SQL that returns a schema change looks like this:\n",
    "```\n",
    "WITH CHANGES AS(\n",
    "    SELECT\n",
    "        DATE,\n",
    "        SCHEMA,\n",
    "        LAG(SCHEMA) OVER(ORDER BY DATE) AS PAST_SCHEMA\n",
    "    FROM\n",
    "        EXOPLANETS_SCHEMA\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    CHANGES\n",
    "WHERE\n",
    "    SCHEMA != PAST_SCHEMA;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "SQL = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joint_anoms = pd.read_sql_query(SQL, conn)\n",
    "joint_anoms = joint_anoms \\\n",
    "    .rename(columns={clmn: clmn.lower() for clmn in joint_anoms.columns})\n",
    "joint_anoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if you can get an output looking like this:\n",
    "![SegmentLocal](../data/assets/ex3img1.png \"segment\")\n",
    "Below, we'll see the \"de-noising\" effect that joining alerts can have, enhancing the clarity of our data observability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(x=all_days, y=h_zero[\"zero_rate\"])\n",
    "for alert in joint_anoms[\"date\"]: fig.add_vline(x=alert, line_color='red')\n",
    "fig.update_xaxes(title=\"Date\")\n",
    "fig.update_yaxes(title=\"Habitability Zero Rate\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a single reported date, `2020-07-19`. Not only have we reduced the number of reports, but we potentially learn something -- that the schema change in `EXOPLANETS` _caused_ the zero rate in `HABITABLES` to spike. By combining data observability pillars, we're one step closer to resolving the problem!\n",
    "\n",
    "![SegmentLocal](../data/assets/comet.gif \"segment\")\n",
    "\n",
    "## 4. Exercise: Diagnosing Another Distribution Issue\n",
    "\n",
    "Here's another quick mystery. It looks like the `HABITABLES` table returns to normal after a while, if we only look at zero rates. But probing into the **volume** of the table reveals something odd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_added = pd.read_sql_query(\"\"\"\n",
    "SELECT\n",
    "    DATE_ADDED,\n",
    "    COUNT(*) AS ROWS_ADDED\n",
    "FROM\n",
    "    HABITABLES\n",
    "GROUP BY\n",
    "    DATE_ADDED\n",
    "\"\"\", conn)\n",
    "rows_added = rows_added \\\n",
    "    .rename(columns={clmn: clmn.lower() for clmn in rows_added.columns}) \\\n",
    "    .set_index(\"date_added\") \\\n",
    "    .reindex(all_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(x=all_days, y=rows_added[\"rows_added\"])\n",
    "fig.update_xaxes(title=\"Date\")\n",
    "fig.update_yaxes(title=\"Rows Added\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The row counts added seem to increase by ~1.5x each day starting around `2020-09-05`. We could detect this using a naive threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_rc_anoms = pd.read_sql_query(\"\"\"\n",
    "WITH ROW_COUNTS AS(\n",
    "    SELECT\n",
    "        DATE_ADDED,\n",
    "        COUNT(*) AS ROWS_ADDED\n",
    "    FROM\n",
    "        HABITABLES\n",
    "    GROUP BY\n",
    "        DATE_ADDED\n",
    ")\n",
    "SELECT\n",
    "    DATE_ADDED\n",
    "FROM\n",
    "    ROW_COUNTS\n",
    "WHERE\n",
    "    ROWS_ADDED > 130 -- this is my \"detection parameter\" - very naive!\n",
    "\"\"\", conn)\n",
    "h_rc_anoms = h_rc_anoms \\\n",
    "    .rename(columns={clmn: clmn.lower() for clmn in h_rc_anoms.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(x=all_days, y=rows_added[\"rows_added\"])\n",
    "for alert in h_rc_anoms[\"date_added\"]: fig.add_vline(x=alert, line_color='red')\n",
    "fig.update_xaxes(title=\"Date\")\n",
    "fig.update_yaxes(title=\"Rows Added\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But again, that's too noisy to be informative. This issue is compounded because volume is usually a problem when it *decreases* (as we saw earlier with freshness). But something must be the cause of this volume change, and turns out, it's a genuine issue.\n",
    "\n",
    "As another exercise in understanding **distribution**, we're going to try querying for _uniqueness_. Uniqueness is pretty simple: for a given field, what % of field values are distinct?\n",
    "\n",
    "Let's take a look at the `HABITABLES` table again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c.execute(\"PRAGMA table_info(HABITABLES);\")\n",
    "c.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While all fields are interesting, we want to pay special attention to the `_id` field. IDs are an interesting piece of the data observability landscape, since they're mostly thought to be unique.\n",
    "\n",
    "As a starting point, see if you can use SQLite's [`DISTINCT` keyword](https://www.tutorialspoint.com/sqlite/sqlite_distinct_keyword.htm) to find the number of distinct `_id`s added per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "SQL = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(SQL, conn).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you get a result looking like this?\n",
    "![SegmentLocal](../data/assets/ex3img2.png \"segment\")\n",
    "If so, great! Let's now try to get the *rate* of `_id` values that are distinct per day. Once again, `CAST(... AS FLOAT)` will be your friend.\n",
    "\n",
    "Also, remember to name your column `PCT_UNIQUE` (or `pct_unique`) so it can be used properly by the visualization code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "SQL = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_uniq = pd.read_sql_query(SQL, conn)\n",
    "h_uniq = h_uniq.rename(columns={clmn: clmn.lower() for clmn in h_uniq.columns}) \\\n",
    "    .set_index(\"date_added\") \\\n",
    "    .reindex(all_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(x=all_days, y=h_uniq[\"pct_unique\"])\n",
    "fig.update_xaxes(title=\"Date\")\n",
    "fig.update_yaxes(title=\"_id Uniqueness\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A proper query here should reveal something telling -- the `_ID` field in `HABITABLES` is not unique, meaning we may be adding duplicate entries to our table! Semantics should dictate that `_ID` be 100% unique. Try writing a query that turns up the offending dates below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "SQL = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_uniq_anoms = pd.read_sql_query(SQL, conn)\n",
    "h_uniq_anoms = h_uniq_anoms \\\n",
    "    .rename(columns={clmn: clmn.lower() for clmn in h_uniq_anoms.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(x=all_days, y=h_uniq[\"pct_unique\"])\n",
    "for alert in h_uniq_anoms[\"date_added\"]: fig.add_vline(x=alert, line_color='red')\n",
    "fig.update_xaxes(title=\"Date\")\n",
    "fig.update_yaxes(title=\"_id Uniqueness\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we've caught the issue... but does this look too noisy to you? :)\n",
    "# Great work!\n",
    "\n",
    "This last exercise revealed that certain pillars of data observability are often conjoined to give meaningful alerts (volume and uniqueness; schema change and downstream distributions; etc.). In the next exercise, we'll look at some terms from machine learning to scale our approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
